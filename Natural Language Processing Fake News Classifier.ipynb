{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # importing regular expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicing regular expressions: re.split() and re.findall()\n",
    "Now you'll get a chance to write some regular expressions to match digits, strings and non-alphanumeric characters. Take a look at my_string first by printing it in the IPython Shell, to determine how you might best match the different steps.\n",
    "\n",
    "Note: It's important to prefix your regex patterns with r to ensure that your patterns are interpreted in the way you want them to. Else, you may encounter problems to do with escape sequences in strings. For example, \"\\n\" in Python is used to indicate a new line, but if you use the r prefix, it will be interpreted as the raw string \"\\n\" - that is, the character \"\\\" followed by the character \"n\" - and not as a new line.\n",
    "\n",
    "The regular expression module re has already been imported for you.\n",
    "\n",
    "Remember from the video that the syntax for the regex library is to always to pass the pattern first, and then the string second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
      "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
      "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n"
     ]
    }
   ],
   "source": [
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Let', 'RegEx', 'Won', 'Can', 'Or']\n"
     ]
    }
   ],
   "source": [
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n"
     ]
    }
   ],
   "source": [
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4', '19']\n"
     ]
    }
   ],
   "source": [
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer for text classification\n",
    "It's time to begin building your text classifier! The data has been loaded into a DataFrame called df. Explore it in the IPython Shell to investigate what columns you can use. The .head() method is particularly informative.\n",
    "\n",
    "In this exercise, you'll use pandas alongside scikit-learn to create a sparse text vectorizer you can use to train and test a simple supervised model. To begin, you'll set up a CountVectorizer and investigate some of its features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"fake_or_real_news.csv\", dtype=str)\n",
    "df = df[['Unnamed: 0','title', 'text','label']]\n",
    "\n",
    "df['title'] = df['title'].astype(str)\n",
    "df['text'] = df['text'].astype(str)\n",
    "df['label'] = df['label'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    object\n",
       "title         object\n",
       "text          object\n",
       "label         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Unnamed: 0                                              title  \\\n",
      "0       8476                       You Can Smell Hillary’s Fear   \n",
      "1      10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
      "2       3608        Kerry to go to Paris in gesture of sympathy   \n",
      "3      10142  Bernie supporters on Twitter erupt in anger ag...   \n",
      "4        875   The Battle of New York: Why This Primary Matters   \n",
      "\n",
      "                                                text label  \n",
      "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
      "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
      "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
      "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
      "4  It's primary day in New York and front-runners...  REAL  \n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Print the head of df\n",
    "print(df.head())\n",
    "\n",
    "# Create a series to store the labels: y\n",
    "y = df.label\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.33, random_state=53)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import CountVectorizer from sklearn.feature_extraction.text and train_test_split from sklearn.model_selection. <br>\n",
    "Create a Series y to use for the labels by assigning the .label attribute of df to y.<br>\n",
    "Using df[\"text\"] (features) and y (labels), create training and test sets using train_test_split(). Use a test_size of 0.33 and a random_state of 53.<br>\n",
    "Create a CountVectorizer object called count_vectorizer. Ensure you specify the keyword argument stop_words=\"english\" so that stop words are removed.<br>\n",
    "Fit and transform the training data X_train using the .fit_transform() method of your CountVectorizer object. Do the same with the test data X_test, except using the .transform() method.<br>\n",
    "Print the first 10 features of the count_vectorizer using its .get_feature_names() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0000', '000000031', '00000031', '0001', '0002', '000ft', '000x', '001']\n"
     ]
    }
   ],
   "source": [
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using only the 'text' column values: count_test \n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names()[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfidfVectorizer for text classification\n",
    "Import TfidfVectorizer from sklearn.feature_extraction.text. <br>\n",
    "Create a TfidfVectorizer object called tfidf_vectorizer. When doing so, specify the keyword arguments stop_words=\"english\" and max_df=0.7.<br>\n",
    "Fit and transform the training data.<br>\n",
    "Transform the test data.<br>\n",
    "Print the first 10 features of tfidf_vectorizer.<br>\n",
    "Print the first 5 vectors of the tfidf training data using slicing on the. <br>\n",
    "A (or array) attribute of tfidf_train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0000', '000000031', '00000031', '0001', '0002', '000ft', '000x', '001']\n",
      "[[0.         0.06262927 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting the vectors\n",
    "To get a better idea of how the vectors work, you'll investigate them by converting them into pandas DataFrames. <br>\n",
    "\n",
    "Create the DataFrames count_df and tfidf_df by using pd.DataFrame() and specifying the values as the first argument and the columns (or features) as the second argument. <br>\n",
    "The values can be accessed by using the .A attribute of, respectively, count_train and tfidf_train. <br>\n",
    "The columns can be accessed using the .get_feature_names() methods of count_vectorizer and tfidf_vectorizer. <br>\n",
    "Print the head of each DataFrame to investigate their structure. This has been done for you. <br>\n",
    "Test if the column names are the same for each DataFrame by creating a new object called difference to see the difference between the columns that count_df has from tfidf_df. Columns can be accessed using the .columns attribute of a DataFrame. Subtract the set of tfidf_df.columns from the set of count_df.columns. <br>\n",
    "Test if the two DataFrames are equivalent by using the .equals() method on count_df with tfidf_df as the argument. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   00  000  0000  000000031  00000031  0001  0002  000ft  000x  001  ...  שתי  \\\n",
      "0   0    7     0          0         0     0     0      0     0    0  ...    0   \n",
      "1   0    0     0          0         0     0     0      0     0    0  ...    0   \n",
      "2   0    0     0          0         0     0     0      0     0    0  ...    0   \n",
      "3   0    0     0          0         0     0     0      0     0    0  ...    0   \n",
      "4   0    0     0          0         0     0     0      0     0    0  ...    0   \n",
      "\n",
      "   תאמצנה  תוצאה  תחל  תיירות  תנותק  תעודת  תתרכז  عربي  ยงade  \n",
      "0       0      0    0       0      0      0      0     0      0  \n",
      "1       0      0    0       0      0      0      0     0      0  \n",
      "2       0      0    0       0      0      0      0     0      0  \n",
      "3       0      0    0       0      0      0      0     0      0  \n",
      "4       0      0    0       0      0      0      0     0      0  \n",
      "\n",
      "[5 rows x 56478 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create the CountVectorizer DataFrame: count_df\n",
    "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Print the head of count_df\n",
    "print(count_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    00       000  0000  000000031  00000031  0001  0002  000ft  000x  001  \\\n",
      "0  0.0  0.062629   0.0        0.0       0.0   0.0   0.0    0.0   0.0  0.0   \n",
      "1  0.0  0.000000   0.0        0.0       0.0   0.0   0.0    0.0   0.0  0.0   \n",
      "2  0.0  0.000000   0.0        0.0       0.0   0.0   0.0    0.0   0.0  0.0   \n",
      "3  0.0  0.000000   0.0        0.0       0.0   0.0   0.0    0.0   0.0  0.0   \n",
      "4  0.0  0.000000   0.0        0.0       0.0   0.0   0.0    0.0   0.0  0.0   \n",
      "\n",
      "   ...  שתי  תאמצנה  תוצאה  תחל  תיירות  תנותק  תעודת  תתרכז  عربي  ยงade  \n",
      "0  ...  0.0     0.0    0.0  0.0     0.0    0.0    0.0    0.0   0.0    0.0  \n",
      "1  ...  0.0     0.0    0.0  0.0     0.0    0.0    0.0    0.0   0.0    0.0  \n",
      "2  ...  0.0     0.0    0.0  0.0     0.0    0.0    0.0    0.0   0.0    0.0  \n",
      "3  ...  0.0     0.0    0.0  0.0     0.0    0.0    0.0    0.0   0.0    0.0  \n",
      "4  ...  0.0     0.0    0.0  0.0     0.0    0.0    0.0    0.0   0.0    0.0  \n",
      "\n",
      "[5 rows x 56478 columns]\n"
     ]
    }
   ],
   "source": [
    "# Print the head of tfidf_df\n",
    "print(tfidf_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Calculate the difference in columns: difference\n",
    "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
    "print(difference)\n",
    "\n",
    "# Check whether the DataFrames are equal\n",
    "print(count_df.equals(tfidf_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing the \"fake news\" model with CountVectorizer\n",
    "Import the metrics module from sklearn and MultinomialNB from sklearn.naive_bayes. <br>\n",
    "Instantiate a MultinomialNB classifier called nb_classifier.<br>\n",
    "Fit the classifier to the training data.<br>\n",
    "Compute the predicted tags for the test data.<br>\n",
    "Calculate and print the accuracy score of the classifier.<br>\n",
    "Compute the confusion matrix. To make it easier to read, specify the keyword argument labels=['FAKE', 'REAL']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8453167508744656"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# Multiclass classification naive bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# fit classifier to the count train and y train\n",
    "nb_classifier.fit(count_train, y_train) # count train is vectorized version of X\n",
    "\n",
    "# use classifier to predict count train\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# Get accuracy\n",
    "metrics.accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[905 146]\n",
      " [ 79 956]]\n",
      "True Positive (TP) : The genere observation is true, and is predicted to be true. It predecited that right 905 times\n",
      "False Negative (FN) : Observation is positive, but is predicted negative. this happened 146 times\n",
      "True Negative (TN) : Observation is negative, and is predicted to be negative. this happened 956 times\n",
      "False Positive (FP) : Observation is negative, but is predicted positive. this happened 79 times\n"
     ]
    }
   ],
   "source": [
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)\n",
    "\n",
    "print(\"True Positive (TP) : The genere observation is true, and is predicted to be true. It predecited that right 905 times\")\n",
    "print(\"False Negative (FN) : Observation is positive, but is predicted negative. this happened 146 times\")\n",
    "print(\"True Negative (TN) : Observation is negative, and is predicted to be negative. this happened 956 times\")\n",
    "print(\"False Positive (FP) : Observation is negative, but is predicted positive. this happened 79 times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing the \"fake news\" model with TfidfVectorizer\n",
    "Now that you have evaluated the model using the CountVectorizer, you'll do the same using the TfidfVectorizer with a Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.792071511853867\n",
      "[[ 713  339]\n",
      " [  21 1017]]\n"
     ]
    }
   ],
   "source": [
    "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving your model\n",
    "Your job in this exercise is to test a few different alpha levels using the Tfidf vectors to determine if there is a better performing combination. <br>\n",
    "\n",
    "Create a list of alphas to try using np.arange(). Values should range from 0 to 1 with steps of 0.1.<br>\n",
    "Create a function train_and_predict() that takes in one argument: alpha. The function should:<br>\n",
    "Instantiate a MultinomialNB classifier with alpha=alpha.<br>\n",
    "Fit it to the training data.<br>\n",
    "Compute predictions on the test data.<br>\n",
    "Compute and return the accuracy score.<br>\n",
    "Using a for loop, print the alpha, score and a newline in between.<br>\n",
    "Use your train_and_predict() function to compute the score. Does the score change along with the alpha? What is the best alpha?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blake\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.8449280994947532\n",
      "\n",
      "Alpha:  0.1\n",
      "Score:  0.8530897784687136\n",
      "\n",
      "Alpha:  0.2\n",
      "Score:  0.8422075398367664\n",
      "\n",
      "Alpha:  0.30000000000000004\n",
      "Score:  0.8359891177613681\n",
      "\n",
      "Alpha:  0.4\n",
      "Score:  0.8286047415468325\n",
      "\n",
      "Alpha:  0.5\n",
      "Score:  0.8208317139525846\n",
      "\n",
      "Alpha:  0.6000000000000001\n",
      "Score:  0.8150019432568986\n",
      "\n",
      "Alpha:  0.7000000000000001\n",
      "Score:  0.8080062184220754\n",
      "\n",
      "Alpha:  0.8\n",
      "Score:  0.8006218422075398\n",
      "\n",
      "Alpha:  0.9\n",
      "Score:  0.7982899339292654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0, 1, .1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best model is with an alpha of .3 above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting your model\n",
    "Now that you have built a \"fake news\" classifier, you'll investigate what it has learned. You can map the important vector weights back to actual words using some simple inspection techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [(-10.941624166627946, '00'), (-10.941624166627946, '000'), (-10.941624166627946, '0000'), (-10.941624166627946, '000000031'), (-10.941624166627946, '00000031'), (-10.941624166627946, '0001'), (-10.941624166627946, '0002'), (-10.941624166627946, '000ft'), (-10.941624166627946, '000x'), (-10.941624166627946, '001'), (-10.941624166627946, '003'), (-10.941624166627946, '004'), (-10.941624166627946, '005'), (-10.941624166627946, '00684'), (-10.941624166627946, '006s'), (-10.941624166627946, '007'), (-10.941624166627946, '007s'), (-10.941624166627946, '008'), (-10.941624166627946, '008s'), (-10.941624166627946, '009')]\n"
     ]
    }
   ],
   "source": [
    "# Get the class labels: class_labels\n",
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1948. [57]  [(-10.941624166627946, 'שינוי'), (-10.941624166627946, 'שיתעקש'), (-10.941624166627946, 'שכל'), (-10.941624166627946, 'שכמוני'), (-10.941624166627946, 'של'), (-10.941624166627946, 'שלו'), (-10.941624166627946, 'שנדרש'), (-10.941624166627946, 'שני'), (-10.941624166627946, 'שעת'), (-10.941624166627946, 'שתי'), (-10.941624166627946, 'תאמצנה'), (-10.941624166627946, 'תוצאה'), (-10.941624166627946, 'תחל'), (-10.941624166627946, 'תיירות'), (-10.941624166627946, 'תנותק'), (-10.941624166627946, 'תעודת'), (-10.941624166627946, 'תתרכז'), (-10.941624166627946, 'عربي'), (-10.941624166627946, 'ยงade'), (-10.248476986068, 'nan')]\n"
     ]
    }
   ],
   "source": [
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-20:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
