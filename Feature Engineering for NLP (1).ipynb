{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib. pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>127447</td>\n",
       "      <td>LIVE STREAM VIDEO=&gt; Donald Trump Rallies in Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123642</td>\n",
       "      <td>Muslim Attacks NYPD Cops with Meat Cleaver. Me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>226970</td>\n",
       "      <td>.@vfpatlas well that's a swella word there (di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>138339</td>\n",
       "      <td>RT wehking_pamela: Bobby_Axelrod2k MMFlint don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>161610</td>\n",
       "      <td>Жители обстреливаемых районов Донецка проводят...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            content\n",
       "0      127447  LIVE STREAM VIDEO=> Donald Trump Rallies in Co...\n",
       "1      123642  Muslim Attacks NYPD Cops with Meat Cleaver. Me...\n",
       "2      226970  .@vfpatlas well that's a swella word there (di...\n",
       "3      138339  RT wehking_pamela: Bobby_Axelrod2k MMFlint don...\n",
       "4      161610  Жители обстреливаемых районов Донецка проводят..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv(\"russian_tweets.csv\")\n",
    "ted = pd.read_csv(\"ted.csv\")\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character count of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103.462\n"
     ]
    }
   ],
   "source": [
    "# Create a feature char_count\n",
    "tweets['char_count'] = tweets['content'].apply(len)\n",
    "\n",
    "# Print the average character count\n",
    "print(tweets['char_count'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word count of TED talks\n",
    "ted is a dataframe that contains the transcripts of 500 TED talks. Your job is to compute a new feature word_count which contains the approximate number of words for each talk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1987.1\n"
     ]
    }
   ],
   "source": [
    "# Function that returns number of words in a string\n",
    "def count_words(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Return the number of words\n",
    "    return len(words)\n",
    "\n",
    "# Create a new feature word_count\n",
    "ted['word_count'] = ted['transcript'].apply(count_words)\n",
    "\n",
    "# Print the average word count of the talks\n",
    "print(ted['word_count'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashtags and mentions in Russian tweets\n",
    "Let's revisit the tweets dataframe containing the Russian tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function that returns number of hashtags in a string\n",
    "def count_hashtags(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words that are hashtags\n",
    "    hashtags = [word for word in words if word.startswith('#')]\n",
    "    \n",
    "    # Return number of hashtags\n",
    "    return(len(hashtags))\n",
    "\n",
    "# Create a feature hashtag_count and display distribution\n",
    "tweets['hashtag_count'] = tweets['content'].apply(count_hashtags)\n",
    "tweets['hashtag_count'].hist()\n",
    "plt.title('Hashtag count distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function that returns number of mentions in a string\n",
    "def count_mentions(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words that are mentions\n",
    "    mentions = [word for word in words if word.startswith('@')]\n",
    "    \n",
    "    # Return number of mentions\n",
    "    return(len(mentions))\n",
    "\n",
    "# Create a feature mention_count and display distribution\n",
    "tweets['mention_count'] = tweets['content'].apply(count_mentions)\n",
    "tweets['mention_count'].hist()\n",
    "plt.title('Mention count distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readability score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textatistic\n",
      "  Downloading textatistic-0.0.1.tar.gz (29 kB)\n",
      "Collecting pyhyphen>=2.0.5\n",
      "  Downloading PyHyphen-3.0.1.tar.gz (31 kB)\n",
      "Collecting appdirs\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: six in c:\\users\\blake\\anaconda3\\lib\\site-packages (from pyhyphen>=2.0.5->textatistic) (1.15.0)\n",
      "Building wheels for collected packages: textatistic, pyhyphen\n",
      "  Building wheel for textatistic (setup.py): started\n",
      "  Building wheel for textatistic (setup.py): finished with status 'done'\n",
      "  Created wheel for textatistic: filename=textatistic-0.0.1-py3-none-any.whl size=29060 sha256=dae7e199b27787da2a6d364d5132639b64d3fc02d4cf1c92098d4c2f848b074c\n",
      "  Stored in directory: c:\\users\\blake\\appdata\\local\\pip\\cache\\wheels\\58\\4a\\1a\\5ed2a089cbd2f98693b07221c4ab499c8c446e15b6123ba4a4\n",
      "  Building wheel for pyhyphen (setup.py): started\n",
      "  Building wheel for pyhyphen (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for pyhyphen\n",
      "Successfully built textatistic\n",
      "Failed to build pyhyphen\n",
      "Installing collected packages: appdirs, pyhyphen, textatistic\n",
      "    Running setup.py install for pyhyphen: started\n",
      "    Running setup.py install for pyhyphen: finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\blake\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\blake\\\\AppData\\\\Local\\\\Temp\\\\pip-install-wvdcs3px\\\\pyhyphen\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\blake\\\\AppData\\\\Local\\\\Temp\\\\pip-install-wvdcs3px\\\\pyhyphen\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\blake\\AppData\\Local\\Temp\\pip-wheel-mj4_otsb'\n",
      "       cwd: C:\\Users\\blake\\AppData\\Local\\Temp\\pip-install-wvdcs3px\\pyhyphen\\\n",
      "  Complete output (17 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.7\n",
      "  creating build\\lib.win-amd64-3.7\\hyphen\n",
      "  copying hyphen\\dictools.py -> build\\lib.win-amd64-3.7\\hyphen\n",
      "  copying hyphen\\hyphenator.py -> build\\lib.win-amd64-3.7\\hyphen\n",
      "  copying hyphen\\__init__.py -> build\\lib.win-amd64-3.7\\hyphen\n",
      "  creating build\\lib.win-amd64-3.7\\textwrap2\n",
      "  copying textwrap2\\cli.py -> build\\lib.win-amd64-3.7\\textwrap2\n",
      "  copying textwrap2\\python2.py -> build\\lib.win-amd64-3.7\\textwrap2\n",
      "  copying textwrap2\\python3.py -> build\\lib.win-amd64-3.7\\textwrap2\n",
      "  copying textwrap2\\__init__.py -> build\\lib.win-amd64-3.7\\textwrap2\n",
      "  running build_ext\n",
      "  building 'hyphen.hnj' extension\n",
      "  error: Microsoft Visual C++ 14.0 is required. Get it with \"Build Tools for Visual Studio\": https://visualstudio.microsoft.com/downloads/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for pyhyphen\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\blake\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\blake\\\\AppData\\\\Local\\\\Temp\\\\pip-install-wvdcs3px\\\\pyhyphen\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\blake\\\\AppData\\\\Local\\\\Temp\\\\pip-install-wvdcs3px\\\\pyhyphen\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\blake\\AppData\\Local\\Temp\\pip-record-guzb1e5e\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\blake\\anaconda3\\Include\\pyhyphen'\n",
      "         cwd: C:\\Users\\blake\\AppData\\Local\\Temp\\pip-install-wvdcs3px\\pyhyphen\\\n",
      "    Complete output (17 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win-amd64-3.7\n",
      "    creating build\\lib.win-amd64-3.7\\hyphen\n",
      "    copying hyphen\\dictools.py -> build\\lib.win-amd64-3.7\\hyphen\n",
      "    copying hyphen\\hyphenator.py -> build\\lib.win-amd64-3.7\\hyphen\n",
      "    copying hyphen\\__init__.py -> build\\lib.win-amd64-3.7\\hyphen\n",
      "    creating build\\lib.win-amd64-3.7\\textwrap2\n",
      "    copying textwrap2\\cli.py -> build\\lib.win-amd64-3.7\\textwrap2\n",
      "    copying textwrap2\\python2.py -> build\\lib.win-amd64-3.7\\textwrap2\n",
      "    copying textwrap2\\python3.py -> build\\lib.win-amd64-3.7\\textwrap2\n",
      "    copying textwrap2\\__init__.py -> build\\lib.win-amd64-3.7\\textwrap2\n",
      "    running build_ext\n",
      "    building 'hyphen.hnj' extension\n",
      "    error: Microsoft Visual C++ 14.0 is required. Get it with \"Build Tools for Visual Studio\": https://visualstudio.microsoft.com/downloads/\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\Users\\blake\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\blake\\\\AppData\\\\Local\\\\Temp\\\\pip-install-wvdcs3px\\\\pyhyphen\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\blake\\\\AppData\\\\Local\\\\Temp\\\\pip-install-wvdcs3px\\\\pyhyphen\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\blake\\AppData\\Local\\Temp\\pip-record-guzb1e5e\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\blake\\anaconda3\\Include\\pyhyphen' Check the logs for full command output.\n"
     ]
    }
   ],
   "source": [
    "pip install textatistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Textatistic\n",
    "from textatistic import Textatistic\n",
    "\n",
    "# Compute the readability scores \n",
    "readability_scores = Textatistic(sisyphus_essay).scores\n",
    "\n",
    "# Print the flesch reading ease score\n",
    "flesch = readability_scores['flesch_score']\n",
    "print(\"The Flesch Reading Ease is %.2f\" % (flesch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Flesch Reading Ease is 81.67\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "forbes = '\\nThe idea is to create more transparency about companies and individuals that are breaking the law or are non-compliant with official obligations and incentivize the right behaviors with the overall goal of improving governance and market order. The Chinese Communist Party intends the social credit score system to “allow the trustworthy to roam freely under heaven while making it hard for the discredited to take a single step.” Even though the system is still under development it currently plays out in real life in myriad ways for private citizens, businesses and government officials. Generally, higher credit scores give people a variety of advantages. Individuals are often given perks such as discounted energy bills and access or better visibility on dating websites. Often, those with higher social credit scores are able to forgo deposits on rental properties, bicycles, and umbrellas. They can even get better travel deals. In addition, Chinese hospitals are currently experimenting with social credit scores. A social credit score above 650 at one hospital allows an individual to see a doctor without lining up to pay.\\n'\n",
    "harvard_law = '\\nThe idea is to create more transparency about companies and individuals that are breaking the law or are non-compliant with official obligations and incentivize the right behaviors with the overall goal of improving governance and market order. The Chinese Communist Party intends the social credit score system to “allow the trustworthy to roam freely under heaven while making it hard for the discredited to take a single step.” Even though the system is still under development it currently plays out in real life in myriad ways for private citizens, businesses and government officials. Generally, higher credit scores give people a variety of advantages. Individuals are often given perks such as discounted energy bills and access or better visibility on dating websites. Often, those with higher social credit scores are able to forgo deposits on rental properties, bicycles, and umbrellas. They can even get better travel deals. In addition, Chinese hospitals are currently experimenting with social credit scores. A social credit score above 650 at one hospital allows an individual to see a doctor without lining up to pay.\\n'\n",
    "r_digest = '\\nThe idea is to create more transparency about companies and individuals that are breaking the law or are non-compliant with official obligations and incentivize the right behaviors with the overall goal of improving governance and market order. The Chinese Communist Party intends the social credit score system to “allow the trustworthy to roam freely under heaven while making it hard for the discredited to take a single step.” Even though the system is still under development it currently plays out in real life in myriad ways for private citizens, businesses and government officials. Generally, higher credit scores give people a variety of advantages. Individuals are often given perks such as discounted energy bills and access or better visibility on dating websites. Often, those with higher social credit scores are able to forgo deposits on rental properties, bicycles, and umbrellas. They can even get better travel deals. In addition, Chinese hospitals are currently experimenting with social credit scores. A social credit score above 650 at one hospital allows an individual to see a doctor without lining up to pay.\\n'\n",
    "time_kids = '\\nThe idea is to create more transparency about companies and individuals that are breaking the law or are non-compliant with official obligations and incentivize the right behaviors with the overall goal of improving governance and market order. The Chinese Communist Party intends the social credit score system to “allow the trustworthy to roam freely under heaven while making it hard for the discredited to take a single step.” Even though the system is still under development it currently plays out in real life in myriad ways for private citizens, businesses and government officials. Generally, higher credit scores give people a variety of advantages. Individuals are often given perks such as discounted energy bills and access or better visibility on dating websites. Often, those with higher social credit scores are able to forgo deposits on rental properties, bicycles, and umbrellas. They can even get better travel deals. In addition, Chinese hospitals are currently experimenting with social credit scores. A social credit score above 650 at one hospital allows an individual to see a doctor without lining up to pay.\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Textatistic\n",
    "from textatistic import Textatistic\n",
    "\n",
    "# List of excerpts\n",
    "excerpts = [forbes, harvard_law, r_digest, time_kids]\n",
    "\n",
    "# Loop through excerpts and compute gunning fog index\n",
    "gunning_fog_scores = []\n",
    "for excerpt in excerpts:\n",
    "  readability_scores = Textatistic(excerpt).scores\n",
    "  gunning_fog = readability_scores['gunningfog_score']\n",
    "  gunning_fog_scores.append(gunning_fog)\n",
    "  \n",
    "# Print the gunning fog indices\n",
    "print(gunning_fog_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[14.436002482929858, 20.735401069518716, 11.085587583148559, 5.926785009861934]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gettysburg = \"Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now we're engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We're met on a great battlefield of that war. We've come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It's altogether fitting and proper that we should do this. But, in a larger sense, we can't dedicate - we can not consecrate - we can not hallow - this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It's rather for us to be here dedicated to the great task remaining before us - that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion - that we here highly resolve that these dead shall not have died in vain - that this nation, under God, shall have a new birth of freedom - and that government of the people, by the people, for the people, shall not perish from the earth.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Four', 'score', 'and', 'seven', 'years', 'ago', 'our', 'fathers', 'brought', 'forth', 'on', 'this', 'continent', ',', 'a', 'new', 'nation', ',', 'conceived', 'in', 'Liberty', ',', 'and', 'dedicated', 'to', 'the', 'proposition', 'that', 'all', 'men', 'are', 'created', 'equal', '.', 'Now', 'we', \"'re\", 'engaged', 'in', 'a', 'great', 'civil', 'war', ',', 'testing', 'whether', 'that', 'nation', ',', 'or', 'any', 'nation', 'so', 'conceived', 'and', 'so', 'dedicated', ',', 'can', 'long', 'endure', '.', 'We', \"'re\", 'met', 'on', 'a', 'great', 'battlefield', 'of', 'that', 'war', '.', 'We', \"'ve\", 'come', 'to', 'dedicate', 'a', 'portion', 'of', 'that', 'field', ',', 'as', 'a', 'final', 'resting', 'place', 'for', 'those', 'who', 'here', 'gave', 'their', 'lives', 'that', 'that', 'nation', 'might', 'live', '.', 'It', \"'s\", 'altogether', 'fitting', 'and', 'proper', 'that', 'we', 'should', 'do', 'this', '.', 'But', ',', 'in', 'a', 'larger', 'sense', ',', 'we', 'ca', \"n't\", 'dedicate', '-', 'we', 'can', 'not', 'consecrate', '-', 'we', 'can', 'not', 'hallow', '-', 'this', 'ground', '.', 'The', 'brave', 'men', ',', 'living', 'and', 'dead', ',', 'who', 'struggled', 'here', ',', 'have', 'consecrated', 'it', ',', 'far', 'above', 'our', 'poor', 'power', 'to', 'add', 'or', 'detract', '.', 'The', 'world', 'will', 'little', 'note', ',', 'nor', 'long', 'remember', 'what', 'we', 'say', 'here', ',', 'but', 'it', 'can', 'never', 'forget', 'what', 'they', 'did', 'here', '.', 'It', 'is', 'for', 'us', 'the', 'living', ',', 'rather', ',', 'to', 'be', 'dedicated', 'here', 'to', 'the', 'unfinished', 'work', 'which', 'they', 'who', 'fought', 'here', 'have', 'thus', 'far', 'so', 'nobly', 'advanced', '.', 'It', \"'s\", 'rather', 'for', 'us', 'to', 'be', 'here', 'dedicated', 'to', 'the', 'great', 'task', 'remaining', 'before', 'us', '-', 'that', 'from', 'these', 'honored', 'dead', 'we', 'take', 'increased', 'devotion', 'to', 'that', 'cause', 'for', 'which', 'they', 'gave', 'the', 'last', 'full', 'measure', 'of', 'devotion', '-', 'that', 'we', 'here', 'highly', 'resolve', 'that', 'these', 'dead', 'shall', 'not', 'have', 'died', 'in', 'vain', '-', 'that', 'this', 'nation', ',', 'under', 'God', ',', 'shall', 'have', 'a', 'new', 'birth', 'of', 'freedom', '-', 'and', 'that', 'government', 'of', 'the', 'people', ',', 'by', 'the', 'people', ',', 'for', 'the', 'people', ',', 'shall', 'not', 'perish', 'from', 'the', 'earth', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(gettysburg)\n",
    "\n",
    "# Generate the tokens\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "four score and seven year ago -PRON- father bring forth on this continent , a new nation , conceive in Liberty , and dedicate to the proposition that all man be create equal . now -PRON- be engage in a great civil war , test whether that nation , or any nation so conceived and so dedicated , can long endure . -PRON- be meet on a great battlefield of that war . -PRON- have come to dedicate a portion of that field , as a final resting place for those who here give -PRON- life that that nation might live . -PRON- be altogether fitting and proper that -PRON- should do this . but , in a large sense , -PRON- can not dedicate - -PRON- can not consecrate - -PRON- can not hallow - this ground . the brave man , living and dead , who struggle here , have consecrate -PRON- , far above -PRON- poor power to add or detract . the world will little note , nor long remember what -PRON- say here , but -PRON- can never forget what -PRON- do here . -PRON- be for -PRON- the living , rather , to be dedicate here to the unfinished work which -PRON- who fight here have thus far so nobly advanced . -PRON- be rather for -PRON- to be here dedicate to the great task remain before -PRON- - that from these honor dead -PRON- take increase devotion to that cause for which -PRON- give the last full measure of devotion - that -PRON- here highly resolve that these dead shall not have die in vain - that this nation , under God , shall have a new birth of freedom - and that government of the people , by the people , for the people , shall not perish from the earth .\n"
     ]
    }
   ],
   "source": [
    "# Create a Doc object\n",
    "doc = nlp(gettysburg)\n",
    "\n",
    "# Generate lemmas\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Convert lemmas into a string\n",
    "print(' '.join(lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog =  '\\nTwenty-first-century politics has witnessed an alarming rise of populism in the U.S. and Europe. The first warning signs came with the UK Brexit Referendum vote in 2016 swinging in the way of Leave. This was followed by a stupendous victory by billionaire Donald Trump to become the 45th President of the United States in November 2016. Since then, Europe has seen a steady rise in populist and far-right parties that have capitalized on Europe’s Immigration Crisis to raise nationalist and anti-Europe sentiments. Some instances include Alternative for Germany (AfD) winning 12.6% of all seats and entering the Bundestag, thus upsetting Germany’s political order for the first time since the Second World War, the success of the Five Star Movement in Italy and the surge in popularity of neo-nazism and neo-fascism in countries such as Hungary, Czech Republic, Poland and Austria.\\n'\n",
    "stopwords = ['fifteen',\n",
    " 'noone',\n",
    " 'whereupon',\n",
    " 'could',\n",
    " 'ten',\n",
    " 'all',\n",
    " 'please',\n",
    " 'indeed',\n",
    " 'whole',\n",
    " 'beside',\n",
    " 'therein',\n",
    " 'using',\n",
    " 'but',\n",
    " 'very',\n",
    " 'already',\n",
    " 'about',\n",
    " 'no',\n",
    " 'regarding',\n",
    " 'afterwards',\n",
    " 'front',\n",
    " 'go',\n",
    " 'in',\n",
    " 'make',\n",
    " 'three',\n",
    " 'here',\n",
    " 'what',\n",
    " 'without',\n",
    " 'yourselves',\n",
    " 'which',\n",
    " 'nothing',\n",
    " 'am',\n",
    " 'between',\n",
    " 'along',\n",
    " 'herein',\n",
    " 'sometimes',\n",
    " 'did',\n",
    " 'as',\n",
    " 'within',\n",
    " 'elsewhere',\n",
    " 'was',\n",
    " 'forty',\n",
    " 'becoming',\n",
    " 'how',\n",
    " 'will',\n",
    " 'other',\n",
    " 'bottom',\n",
    " 'these',\n",
    " 'amount',\n",
    " 'across',\n",
    " 'the',\n",
    " 'than',\n",
    " 'first',\n",
    " 'namely',\n",
    " 'may',\n",
    " 'none',\n",
    " 'anyway',\n",
    " 'again',\n",
    " 'eleven',\n",
    " 'his',\n",
    " 'meanwhile',\n",
    " 'name',\n",
    " 're',\n",
    " 'from',\n",
    " 'some',\n",
    " 'thru',\n",
    " 'upon',\n",
    " 'whither',\n",
    " 'he',\n",
    " 'such',\n",
    " 'down',\n",
    " 'my',\n",
    " 'often',\n",
    " 'whether',\n",
    " 'made',\n",
    " 'while',\n",
    " 'empty',\n",
    " 'two',\n",
    " 'latter',\n",
    " 'whatever',\n",
    " 'cannot',\n",
    " 'less',\n",
    " 'many',\n",
    " 'you',\n",
    " 'ours',\n",
    " 'done',\n",
    " 'thus',\n",
    " 'since',\n",
    " 'everything',\n",
    " 'for',\n",
    " 'more',\n",
    " 'unless',\n",
    " 'former',\n",
    " 'anyone',\n",
    " 'per',\n",
    " 'seeming',\n",
    " 'hereafter',\n",
    " 'on',\n",
    " 'yours',\n",
    " 'always',\n",
    " 'due',\n",
    " 'last',\n",
    " 'alone',\n",
    " 'one',\n",
    " 'something',\n",
    " 'twenty',\n",
    " 'until',\n",
    " 'latterly',\n",
    " 'seems',\n",
    " 'were',\n",
    " 'where',\n",
    " 'eight',\n",
    " 'ourselves',\n",
    " 'further',\n",
    " 'themselves',\n",
    " 'therefore',\n",
    " 'they',\n",
    " 'whenever',\n",
    " 'after',\n",
    " 'among',\n",
    " 'when',\n",
    " 'at',\n",
    " 'through',\n",
    " 'put',\n",
    " 'thereby',\n",
    " 'then',\n",
    " 'should',\n",
    " 'formerly',\n",
    " 'third',\n",
    " 'who',\n",
    " 'this',\n",
    " 'neither',\n",
    " 'others',\n",
    " 'twelve',\n",
    " 'also',\n",
    " 'else',\n",
    " 'seemed',\n",
    " 'has',\n",
    " 'ever',\n",
    " 'someone',\n",
    " 'its',\n",
    " 'that',\n",
    " 'does',\n",
    " 'sixty',\n",
    " 'why',\n",
    " 'do',\n",
    " 'whereas',\n",
    " 'are',\n",
    " 'either',\n",
    " 'hereupon',\n",
    " 'rather',\n",
    " 'because',\n",
    " 'might',\n",
    " 'those',\n",
    " 'via',\n",
    " 'hence',\n",
    " 'itself',\n",
    " 'show',\n",
    " 'perhaps',\n",
    " 'various',\n",
    " 'during',\n",
    " 'otherwise',\n",
    " 'thereafter',\n",
    " 'yourself',\n",
    " 'become',\n",
    " 'now',\n",
    " 'same',\n",
    " 'enough',\n",
    " 'been',\n",
    " 'take',\n",
    " 'their',\n",
    " 'seem',\n",
    " 'there',\n",
    " 'next',\n",
    " 'above',\n",
    " 'mostly',\n",
    " 'once',\n",
    " 'a',\n",
    " 'top',\n",
    " 'almost',\n",
    " 'six',\n",
    " 'every',\n",
    " 'nobody',\n",
    " 'any',\n",
    " 'say',\n",
    " 'each',\n",
    " 'them',\n",
    " 'must',\n",
    " 'she',\n",
    " 'throughout',\n",
    " 'whence',\n",
    " 'hundred',\n",
    " 'not',\n",
    " 'however',\n",
    " 'together',\n",
    " 'several',\n",
    " 'myself',\n",
    " 'i',\n",
    " 'anything',\n",
    " 'somehow',\n",
    " 'or',\n",
    " 'used',\n",
    " 'keep',\n",
    " 'much',\n",
    " 'thereupon',\n",
    " 'ca',\n",
    " 'just',\n",
    " 'behind',\n",
    " 'can',\n",
    " 'becomes',\n",
    " 'me',\n",
    " 'had',\n",
    " 'only',\n",
    " 'back',\n",
    " 'four',\n",
    " 'somewhere',\n",
    " 'if',\n",
    " 'by',\n",
    " 'whereafter',\n",
    " 'everywhere',\n",
    " 'beforehand',\n",
    " 'well',\n",
    " 'doing',\n",
    " 'everyone',\n",
    " 'nor',\n",
    " 'five',\n",
    " 'wherein',\n",
    " 'so',\n",
    " 'amongst',\n",
    " 'though',\n",
    " 'still',\n",
    " 'move',\n",
    " 'except',\n",
    " 'see',\n",
    " 'us',\n",
    " 'your',\n",
    " 'against',\n",
    " 'although',\n",
    " 'is',\n",
    " 'became',\n",
    " 'call',\n",
    " 'have',\n",
    " 'most',\n",
    " 'wherever',\n",
    " 'few',\n",
    " 'out',\n",
    " 'whom',\n",
    " 'yet',\n",
    " 'be',\n",
    " 'own',\n",
    " 'off',\n",
    " 'quite',\n",
    " 'with',\n",
    " 'and',\n",
    " 'side',\n",
    " 'whoever',\n",
    " 'would',\n",
    " 'both',\n",
    " 'fifty',\n",
    " 'before',\n",
    " 'full',\n",
    " 'get',\n",
    " 'sometime',\n",
    " 'beyond',\n",
    " 'part',\n",
    " 'least',\n",
    " 'besides',\n",
    " 'around',\n",
    " 'even',\n",
    " 'whose',\n",
    " 'hereby',\n",
    " 'up',\n",
    " 'being',\n",
    " 'we',\n",
    " 'an',\n",
    " 'him',\n",
    " 'below',\n",
    " 'moreover',\n",
    " 'really',\n",
    " 'it',\n",
    " 'of',\n",
    " 'our',\n",
    " 'nowhere',\n",
    " 'whereby',\n",
    " 'too',\n",
    " 'her',\n",
    " 'toward',\n",
    " 'anyhow',\n",
    " 'give',\n",
    " 'never',\n",
    " 'another',\n",
    " 'anywhere',\n",
    " 'mine',\n",
    " 'herself',\n",
    " 'over',\n",
    " 'himself',\n",
    " 'to',\n",
    " 'onto',\n",
    " 'into',\n",
    " 'thence',\n",
    " 'towards',\n",
    " 'hers',\n",
    " 'nevertheless',\n",
    " 'serious',\n",
    " 'under',\n",
    " 'nine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "century politic witness alarming rise populism Europe warning sign come UK Brexit Referendum vote swinge way Leave follow stupendous victory billionaire Donald Trump President United States November Europe steady rise populist far right party capitalize Europe Immigration Crisis raise nationalist anti europe sentiment instance include Alternative Germany AfD win seat enter Bundestag upset Germany political order time Second World War success Five Star Movement Italy surge popularity neo nazism neo fascism country Hungary Czech Republic Poland Austria\n"
     ]
    }
   ],
   "source": [
    "# Load model and create Doc object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(blog)\n",
    "\n",
    "# Generate lemmatized tokens\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Remove stopwords and non-alphabetic tokens\n",
    "a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "\n",
    "# Print string after text cleaning\n",
    "print(' '.join(a_lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning TED talks in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      talk new lecture TED illusion create TED try r...\n",
      "1      representation brain brain break left half log...\n",
      "2      great honor today share Digital Universe creat...\n",
      "3      passion music technology thing combination thi...\n",
      "4      use want computer new program programming requ...\n",
      "                             ...                        \n",
      "495    today unpack example iconic design perfect sen...\n",
      "496    brother belong demographic Pat percent accord ...\n",
      "497    John Hockenberry great Tom want start question...\n",
      "498    right moment kill More car internet little mob...\n",
      "499    real problem math education right basically ha...\n",
      "Name: transcript, Length: 500, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess(text):\n",
    "  \t# Create Doc object\n",
    "    doc = nlp(text, disable=['ner', 'parser'])\n",
    "    # Generate lemmas\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    # Remove stopwords and non-alphabetic characters\n",
    "    a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "    \n",
    "    return ' '.join(a_lemmas)\n",
    "  \n",
    "# Apply preprocess to ted['transcript']\n",
    "ted['transcript'] = ted['transcript'].apply(preprocess)\n",
    "print(ted['transcript'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lotf = 'He found himself understanding the wearisomeness of this life, where every path was an improvisation and a considerable part of one’s waking life was spent watching one’s feet.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 'PRON'), ('found', 'VERB'), ('himself', 'PRON'), ('understanding', 'VERB'), ('the', 'DET'), ('wearisomeness', 'NOUN'), ('of', 'ADP'), ('this', 'DET'), ('life', 'NOUN'), (',', 'PUNCT'), ('where', 'ADV'), ('every', 'DET'), ('path', 'NOUN'), ('was', 'AUX'), ('an', 'DET'), ('improvisation', 'NOUN'), ('and', 'CCONJ'), ('a', 'DET'), ('considerable', 'ADJ'), ('part', 'NOUN'), ('of', 'ADP'), ('one', 'NUM'), ('’s', 'NOUN'), ('waking', 'VERB'), ('life', 'NOUN'), ('was', 'AUX'), ('spent', 'VERB'), ('watching', 'VERB'), ('one', 'NUM'), ('’s', 'PART'), ('feet', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(lotf)\n",
    "\n",
    "# Generate tokens and pos tags\n",
    "pos = [(token.text, token.pos_) for token in doc]\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting nouns in a piece of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Returns number of proper nouns\n",
    "def proper_nouns(text, model=nlp):\n",
    "  \t# Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of proper nouns\n",
    "    return pos.count('PROPN')\n",
    "\n",
    "print(proper_nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count nouns now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Returns number of other nouns\n",
    "def nouns(text, model=nlp):\n",
    "  \t# Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of other nouns\n",
    "    return pos.count('NOUN')\n",
    "\n",
    "print(nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noun usage in fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title label\n",
       "0           0                       You Can Smell Hillary’s Fear  FAKE\n",
       "1           1  Watch The Exact Moment Paul Ryan Committed Pol...  FAKE\n",
       "2           2        Kerry to go to Paris in gesture of sympathy  REAL\n",
       "3           3  Bernie supporters on Twitter erupt in anger ag...  FAKE\n",
       "4           4   The Battle of New York: Why This Primary Matters  REAL"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines = pd.read_csv(\"fakenews.csv\")\n",
    "headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean no. of proper nouns in real and fake headlines are 2.49 and 4.28 respectively\n",
      "Mean no. of other nouns in real and fake headlines are 2.26 and 1.95 respectively\n"
     ]
    }
   ],
   "source": [
    "headlines['num_propn'] = headlines['title'].apply(proper_nouns)\n",
    "headlines['num_noun'] = headlines['title'].apply(nouns)\n",
    "\n",
    "# Compute mean of proper nouns\n",
    "real_propn = headlines[headlines['label'] == 'REAL']['num_propn'].mean()\n",
    "fake_propn = headlines[headlines['label'] == 'FAKE']['num_propn'].mean()\n",
    "\n",
    "# Compute mean of other nouns\n",
    "real_noun = headlines[headlines['label'] == 'REAL']['num_noun'].mean()\n",
    "fake_noun = headlines[headlines['label'] == 'FAKE']['num_noun'].mean()\n",
    "\n",
    "# Print results\n",
    "print(\"Mean no. of proper nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_propn, fake_propn))\n",
    "print(\"Mean no. of other nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_noun, fake_noun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name entity recognition\n",
    "identify and classify the labels of various named entities in a body of text using one of spaCy's statistical models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blake Tindol PERSON\n",
      "Michigan GPE\n",
      "JPG Resources ORG\n"
     ]
    }
   ],
   "source": [
    "# Load the required model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc instance \n",
    "text = 'Blake Tindol is from Michigan and works at JPG Resources as a supply chain analyst.'\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print all named entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying people mentioned in a news article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = \"It’s' been a busy day for Facebook  exec op-eds. Earlier this morning, Sheryl Sandberg broke the site’s silence around the Christchurch massacre, and now Mark Zuckerberg is calling on governments and other bodies to increase regulation around the sorts of data Facebook traffics in. He’s hoping to get out in front of heavy-handed regulation and get a seat at the table shaping it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sheryl Sandberg', 'Mark Zuckerberg']\n"
     ]
    }
   ],
   "source": [
    "def find_persons(text):\n",
    "  # Create Doc object\n",
    "  doc = nlp(text)\n",
    "  \n",
    "  # Identify the persons\n",
    "  persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "  \n",
    "  # Return persons\n",
    "  return persons\n",
    "\n",
    "print(find_persons(tc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a bag of words model\n",
    "BoW model for movie taglines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ted['transcript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      talk new lecture TED illusion create TED try r...\n",
       "1      representation brain brain break left half log...\n",
       "2      great honor today share Digital Universe creat...\n",
       "3      passion music technology thing combination thi...\n",
       "4      use want computer new program programming requ...\n",
       "                             ...                        \n",
       "495    today unpack example iconic design perfect sen...\n",
       "496    brother belong demographic Pat percent accord ...\n",
       "497    John Hockenberry great Tom want start question...\n",
       "498    right moment kill More car internet little mob...\n",
       "499    real problem math education right basically ha...\n",
       "Name: transcript, Length: 500, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 21803)\n"
     ]
    }
   ],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Print the shape of bow_matrix\n",
    "print(bow_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping feature indices with feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     aa  aaa  aah  aak  aaleh  aaron  aarp  ab  ababa  abacha  ...  zyberk  \\\n",
      "0     0    0    0    0      0      0     0   0      0       0  ...       0   \n",
      "1     0    0    0    0      0      0     0   0      0       0  ...       0   \n",
      "2     0    0    0    0      0      0     0   0      0       0  ...       0   \n",
      "3     0    0    0    0      0      0     0   0      0       0  ...       0   \n",
      "4     0    0    0    0      0      0     0   0      0       0  ...       0   \n",
      "..   ..  ...  ...  ...    ...    ...   ...  ..    ...     ...  ...     ...   \n",
      "495   0    0    0    0      0      0     0   0      0       0  ...       0   \n",
      "496   0    0    0    0      0      0     0   0      0       0  ...       0   \n",
      "497   0    0    0    0      0      0     0   0      0       0  ...       0   \n",
      "498   0    0    0    0      0      0     0   0      0       0  ...       0   \n",
      "499   0    0    0    0      0      0     0   0      0       0  ...       0   \n",
      "\n",
      "     zynga  zyprexa  zywiec  ºf  čapek  ʾan  ʾilla  ʾilāha  送你葱  \n",
      "0        0        0       0   0      0    0      0       0    0  \n",
      "1        0        0       0   0      0    0      0       0    0  \n",
      "2        0        0       0   0      0    0      0       0    0  \n",
      "3        0        0       0   0      0    0      0       0    0  \n",
      "4        0        0       0   0      0    0      0       0    0  \n",
      "..     ...      ...     ...  ..    ...  ...    ...     ...  ...  \n",
      "495      0        0       0   0      0    0      0       0    0  \n",
      "496      0        0       0   0      0    0      0       0    0  \n",
      "497      0        0       0   0      0    0      0       0    0  \n",
      "498      0        0       0   0      0    0      0       0    0  \n",
      "499      0        0       0   0      0    0      0       0    0  \n",
      "\n",
      "[500 rows x 21803 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert bow_matrix into a DataFrame\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray())\n",
    "\n",
    "# Map the column names to vocabulary \n",
    "bow_df.columns = vectorizer.get_feature_names()\n",
    "\n",
    "# Print bow_df\n",
    "print(bow_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this anime series starts out great interesting...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>some may go for a film like this but i most as...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i ve seen this piece of perfection during the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this movie is likely the worst movie i ve ever...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it ll soon be 10 yrs since this movie was rele...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  this anime series starts out great interesting...          0\n",
       "1  some may go for a film like this but i most as...          0\n",
       "2  i ve seen this piece of perfection during the ...          1\n",
       "3  this movie is likely the worst movie i ve ever...          0\n",
       "4  it ll soon be 10 yrs since this movie was rele...          1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "reviews = pd.read_csv(\"movie_reviews_clean.csv\")\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = reviews['review']\n",
    "y = reviews['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 15704)\n",
      "(200, 15704)\n"
     ]
    }
   ],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
    "\n",
    "# Fit and transform X_train\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform X_test\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Print shape of X_train_bow and X_test_bow\n",
    "print(X_train_bow.shape)\n",
    "print(X_test_bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the sentiment of a movie review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier on the test set is 0.790\n",
      "The sentiment predicted by the classifier is 0\n"
     ]
    }
   ],
   "source": [
    "# Create a MultinomialNB object\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Fit the classifier\n",
    "clf.fit(X_train_bow, y_train)\n",
    "\n",
    "# Measure the accuracy\n",
    "accuracy = clf.score(X_test_bow, y_test)\n",
    "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
    "\n",
    "# Predict the sentiment of a negative review\n",
    "review = \"The movie was terrible. The music was underwhelming and the acting mediocre.\"\n",
    "prediction = clf.predict(vectorizer.transform([review]))[0]\n",
    "print(\"The sentiment predicted by the classifier is %i\" % (prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building n-gram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = reviews['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng1, ng2 and ng3 have 17782, 134432 and 322694 features respectively\n"
     ]
    }
   ],
   "source": [
    "# Generate n-grams upto n=1\n",
    "vectorizer_ng1 = CountVectorizer(ngram_range=(1,1))\n",
    "ng1 = vectorizer_ng1.fit_transform(corpus)\n",
    "\n",
    "# Generate n-grams upto n=2\n",
    "vectorizer_ng2 = CountVectorizer(ngram_range=(1,2))\n",
    "ng2 = vectorizer_ng2.fit_transform(corpus)\n",
    "\n",
    "# Generate n-grams upto n=3\n",
    "vectorizer_ng3 = CountVectorizer(ngram_range=(1,3))\n",
    "ng3 = vectorizer_ng3.fit_transform(corpus)\n",
    "\n",
    "# Print the number of features for each model\n",
    "print(\"ng1, ng2 and ng3 have %i, %i and %i features respectively\" % (ng1.shape[1], ng2.shape[1], ng3.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higher order n-grams for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ng1\n",
    "y = reviews['sentiment']\n",
    "\n",
    "X_train_ng, X_test_ng, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier on the test set is 0.835\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ng_vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-9e40506944c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Predict the sentiment of a negative review\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mreview\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"The movie was not good. The plot had several holes and the acting lacked panache.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf_ng\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mng_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The sentiment predicted by the classifier is %i\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ng_vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Define an instance of MultinomialNB \n",
    "clf_ng = MultinomialNB()\n",
    "\n",
    "# Fit the classifier\n",
    "clf_ng.fit(X_train_ng, y_train)\n",
    "\n",
    "# Measure the accuracy\n",
    "accuracy = clf_ng.score(X_test_ng, y_test)\n",
    "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
    "\n",
    "# Predict the sentiment of a negative review\n",
    "review = \"The movie was not good. The plot had several holes and the acting lacked panache.\"\n",
    "prediction = clf_ng.predict(ng_vectorizer.transform([review]))[0]\n",
    "print(\"The sentiment predicted by the classifier is %i\" % (prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing performance of n-gram models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The program took 0.223 seconds to complete. The accuracy on the test set is 0.75. The ngram representation had 12347 features.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# make df reviews\n",
    "df = reviews\n",
    "\n",
    "start_time = time.time()\n",
    "# Splitting the data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(df['review'], df['sentiment'], test_size=0.5, random_state=42, stratify=df['sentiment'])\n",
    "\n",
    "# Generating ngrams\n",
    "vectorizer = CountVectorizer()\n",
    "train_X = vectorizer.fit_transform(train_X)\n",
    "test_X = vectorizer.transform(test_X)\n",
    "\n",
    "# Fit classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "# Print accuracy, time and number of dimensions\n",
    "print(\"The program took %.3f seconds to complete. The accuracy on the test set is %.2f. The ngram representation had %i features.\" % (time.time() - start_time, clf.score(test_X, test_y), train_X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The program took 1.152 seconds to complete. The accuracy on the test set is 0.77. The ngram representation had 178240 features.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Splitting the data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(df['review'], df['sentiment'], test_size=0.5, random_state=42, stratify=df['sentiment'])\n",
    "\n",
    "# Generating ngrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
    "train_X = vectorizer.fit_transform(train_X)\n",
    "test_X = vectorizer.transform(test_X)\n",
    "\n",
    "# Fit classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "# Print accuracy, time and number of dimensions\n",
    "print(\"The program took %.3f seconds to complete. The accuracy on the test set is %.2f. The ngram representation had %i features.\" % (time.time() - start_time, clf.score(test_X, test_y), train_X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
